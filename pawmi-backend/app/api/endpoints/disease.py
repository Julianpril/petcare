"""
Endpoints para predicci√≥n de enfermedades
"""
import logging
from typing import Any, Dict, List

from app.schemas.disease import (DiseasePredictionError,
                                 DiseasePredictionRequest,
                                 DiseasePredictionResponse)
from app.services.disease_prediction import disease_service
from app.services.ollama_service import ollama_service
from fastapi import APIRouter, Depends, HTTPException
from pydantic import BaseModel

logger = logging.getLogger(__name__)

router = APIRouter()


@router.post(
    "/predict",
    response_model=DiseasePredictionResponse,
    summary="Predecir enfermedad basada en s√≠ntomas",
    description="""
    Predice la enfermedad m√°s probable bas√°ndose en:
    - Datos demogr√°ficos (especie, edad, tama√±o)
    - Signos vitales (temperatura, frecuencia card√≠aca, etc.)
    - S√≠ntomas cl√≠nicos (v√≥mitos, diarrea, fiebre, etc.)
    
    Retorna las top 3 enfermedades m√°s probables con sus probabilidades.
    
    **‚ö†Ô∏è IMPORTANTE:** Esta es una herramienta de asistencia. 
    Siempre consulte con un veterinario profesional para diagn√≥stico definitivo.
    """,
    tags=["Disease Prediction"]
)
async def predict_disease(
    request: DiseasePredictionRequest
) -> DiseasePredictionResponse:
    """
    Endpoint para predicci√≥n de enfermedades
    
    Args:
        request: Datos del paciente y s√≠ntomas
        
    Returns:
        DiseasePredictionResponse con predicciones
        
    Raises:
        HTTPException: Si ocurre error en predicci√≥n
    """
    try:
        logger.info(f"Nueva solicitud de predicci√≥n para {request.animal_type}, edad {request.age}")
        
        # Convertir request a diccionario
        input_data = request.model_dump()
        
        # Realizar predicci√≥n
        result = await disease_service.predict_disease(input_data)
        
        if not result["success"]:
            raise HTTPException(
                status_code=500,
                detail=result.get("message", "Error en predicci√≥n")
            )
        
        logger.info(f"Predicci√≥n exitosa: {result['predictions'][0]['disease']} ({result['predictions'][0]['probability']*100:.1f}%)")
        
        return DiseasePredictionResponse(**result)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error inesperado en predicci√≥n: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Error interno en predicci√≥n: {str(e)}"
        )


@router.get(
    "/model-info",
    summary="Informaci√≥n del modelo de predicci√≥n",
    description="Retorna informaci√≥n sobre el modelo ML usado para predicciones",
    tags=["Disease Prediction"]
)
async def get_model_info():
    """
    Obtiene informaci√≥n del modelo cargado
    
    Returns:
        Dict con informaci√≥n del modelo
    """
    if not disease_service.model_loaded:
        # Intentar cargar modelo
        success = disease_service.load_model()
        if not success:
            raise HTTPException(
                status_code=503,
                detail="Modelo no disponible. Contacte al administrador."
            )
    
    return {
        "model_loaded": disease_service.model_loaded,
        "model_version": "3.0",
        "model_type": str(type(disease_service.model).__name__) if disease_service.model else None,
        "num_features": len(disease_service.features_info['feature_names']) if disease_service.features_info else None,
        "test_accuracy": disease_service.features_info.get('test_accuracy') if disease_service.features_info else None,
        "cv_score": disease_service.features_info.get('cv_mean') if disease_service.features_info else None,
        "description": "Modelo de predicci√≥n de enfermedades veterinarias entrenado con 9,000+ casos cl√≠nicos"
    }


@router.post(
    "/symptoms-check",
    summary="Verificar sintomatolog√≠a",
    description="An√°lisis r√°pido de s√≠ntomas sin predicci√≥n completa",
    tags=["Disease Prediction"]
)
async def check_symptoms(symptoms: dict):
    """
    Verifica los s√≠ntomas y da recomendaciones generales
    
    Args:
        symptoms: Diccionario con s√≠ntomas activos
        
    Returns:
        An√°lisis y recomendaciones
    """
    active_symptoms = [k for k, v in symptoms.items() if v == 1]
    num_symptoms = len(active_symptoms)
    
    # Clasificar urgencia
    urgent_symptoms = ['diarrea_hemorragica', 'convulsiones', 'disnea', 'ictericia']
    has_urgent = any(s in active_symptoms for s in urgent_symptoms)
    
    if has_urgent:
        urgency = "ALTA - Consulta veterinaria URGENTE"
        recommendation = "Dir√≠gete inmediatamente a un centro veterinario de urgencias"
    elif num_symptoms >= 5:
        urgency = "MEDIA-ALTA - Consulta pronto"
        recommendation = "Programa una cita veterinaria en las pr√≥ximas 24-48 horas"
    elif num_symptoms >= 3:
        urgency = "MEDIA - Monitorear"
        recommendation = "Monitorea la evoluci√≥n y consulta si empeora"
    else:
        urgency = "BAJA - Observaci√≥n"
        recommendation = "Observa a tu mascota. Consulta si aparecen m√°s s√≠ntomas"
    
    return {
        "num_symptoms": num_symptoms,
        "active_symptoms": active_symptoms,
        "urgency_level": urgency,
        "recommendation": recommendation,
        "warning": "Esta es una evaluaci√≥n preliminar. Siempre consulta con un veterinario."
    }


@router.post(
    "/test-prediction",
    summary="üß™ Test: Predicci√≥n con datos de ejemplo",
    description="""
    **Endpoint de prueba** para verificar que el modelo ML est√° funcionando correctamente.
    
    Usa datos de ejemplo de un perro con s√≠ntomas gastrointestinales y retorna:
    - Las predicciones del modelo ML real
    - Informaci√≥n detallada del modelo (accuracy, features, tipo)
    - Todas las clases que el modelo puede predecir
    
    **Esto demuestra que NO hay c√≥digo hardcodeado y que el modelo entrenado est√° respondiendo.**
    """,
    tags=["Disease Prediction"]
)
async def test_prediction():
    """
    Endpoint de prueba con datos de ejemplo para verificar que el modelo funciona
    """
    try:
        # Datos de ejemplo: Perro con s√≠ntomas gastrointestinales
        test_data = {
            "animal_type": "dog",
            "age": 5,
            "size": "medium",
            "weight": 15.0,
            "temperature": 39.5,
            "heart_rate": 120,
            "respiratory_rate": 30,
            "vomiting": True,
            "diarrhea": True,
            "lethargy": True,
            "loss_of_appetite": True,
            "abdominal_pain": True,
            "fever": True,
            "dehydration": False,
            "cough": False,
            "sneezing": False,
            "nasal_discharge": False,
            "eye_discharge": False,
            "itching": False,
            "hair_loss": False,
            "skin_lesions": False,
            "lameness": False,
            "seizures": False,
            "difficulty_breathing": False,
            "increased_thirst": False,
            "increased_urination": False,
            "bloody_stool": False,
            "bloody_vomit": False,
            "weight_loss": False,
            "aggression": False,
            "disorientation": False,
            "jaundice": False,
            "pale_gums": False,
            "swelling": False,
            "disease_cause": "viral",
            "prognosis": "good",
            "fever_objective": 1.0,
            "tachycardia": 1.0,
            "is_chronic": 0.0,
            "is_seasonal": 0.0,
            "prevalence": 0.5,
            "vaccination_updated": 1.0
        }
        
        # Realizar predicci√≥n
        result = await disease_service.predict_disease(test_data)
        
        if not result["success"]:
            return {
                "test_status": "FAILED",
                "error": result.get("message", "Error desconocido"),
                "model_loaded": disease_service.model_loaded
            }
        
        # Agregar informaci√≥n extra de prueba
        all_classes = list(disease_service.model.classes_) if disease_service.model else []
        
        return {
            "test_status": "SUCCESS ‚úÖ",
            "message": "El modelo ML est√° funcionando correctamente",
            "test_case": "Perro con s√≠ntomas gastrointestinales (v√≥mito, diarrea, fiebre)",
            "predictions": result["predictions"],
            "model_verification": {
                "model_loaded": disease_service.model_loaded,
                "model_type": result["model_info"]["model_type"],
                "test_accuracy": result["model_info"]["accuracy"],
                "cv_accuracy": result["model_info"]["cv_mean"],
                "total_features": result["model_info"]["features_used"],
                "total_classes": result["model_info"]["total_classes"],
                "all_disease_classes": all_classes[:10],  # Primeras 10 clases
                "note": "Estas son las clases REALES del modelo entrenado, no hardcodeadas"
            },
            "proof": {
                "description": "Si ves diferentes probabilidades y enfermedades al cambiar s√≠ntomas, es el modelo ML real",
                "test": "Prueba con diferentes s√≠ntomas y ver√°s predicciones diferentes",
                "source": "Modelo entrenado en pawmi-ml/notebooks/03_disease_prediction_dataset3_0.ipynb"
            }
        }
        
    except Exception as e:
        logger.error(f"Error en test de predicci√≥n: {str(e)}")
        return {
            "test_status": "ERROR",
            "error": str(e),
            "model_loaded": disease_service.model_loaded if hasattr(disease_service, 'model_loaded') else False
        }


# ============= ENDPOINTS CON OLLAMA =============

class ExtractSymptomsRequest(BaseModel):
    """Request para extraer s√≠ntomas usando Ollama"""
    text: str
    
class ExtractSymptomsResponse(BaseModel):
    """Response con s√≠ntomas extra√≠dos"""
    symptoms: Dict[str, int]
    symptoms_list: List[str]
    original_text: str


@router.post(
    "/extract-symptoms-ollama",
    response_model=ExtractSymptomsResponse,
    summary="Extraer s√≠ntomas usando Ollama",
    description="""
    Usa Ollama (modelo local de IA) para extraer s√≠ntomas del texto del usuario.
    Es m√°s inteligente que la detecci√≥n por palabras clave, entiende contexto.
    """,
    tags=["Disease Prediction", "Ollama"]
)
async def extract_symptoms_with_ollama(request: ExtractSymptomsRequest):
    """
    Extrae s√≠ntomas del texto usando Ollama para mejor comprensi√≥n
    """
    try:
        symptoms = ollama_service.extract_symptoms_from_text(request.text)
        symptoms_list = [k for k, v in symptoms.items() if v == 1]
        
        return ExtractSymptomsResponse(
            symptoms=symptoms,
            symptoms_list=symptoms_list,
            original_text=request.text
        )
    except Exception as e:
        logger.error(f"Error en extracci√≥n con Ollama: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Error al extraer s√≠ntomas: {str(e)}"
        )


class PredictWithOllamaRequest(BaseModel):
    """Request para predicci√≥n completa con Ollama"""
    user_message: str
    pet_data: DiseasePredictionRequest
    pet_name: str = "tu mascota"


class PredictWithOllamaResponse(BaseModel):
    """Response con predicci√≥n y texto formateado por Ollama"""
    success: bool
    predictions: List[Dict[str, Any]]
    conversational_response: str
    symptoms_detected: Dict[str, int]
    model_info: Dict[str, Any]


@router.post(
    "/predict-with-ollama",
    response_model=PredictWithOllamaResponse,
    summary="Predicci√≥n completa con Ollama",
    description="""
    Pipeline completo que:
    1. Usa Ollama para extraer s√≠ntomas del mensaje del usuario
    2. Llama al modelo ML para predicci√≥n
    3. Usa Ollama para formatear la respuesta de forma conversacional
    
    El modelo ML NO cambia, Ollama solo mejora comprensi√≥n y comunicaci√≥n.
    """,
    tags=["Disease Prediction", "Ollama"]
)
async def predict_with_ollama(request: PredictWithOllamaRequest):
    """
    Predicci√≥n completa usando Ollama como intermediario inteligente
    """
    try:
        # 1. Extraer s√≠ntomas con Ollama
        logger.info(f"Extrayendo s√≠ntomas con Ollama: {request.user_message}")
        symptoms = ollama_service.extract_symptoms_from_text(request.user_message)
        symptoms_list = [k for k, v in symptoms.items() if v == 1]
        
        logger.info(f"S√≠ntomas detectados: {symptoms_list}")
        
        # 2. Actualizar datos del pet con s√≠ntomas detectados
        pet_dict = request.pet_data.model_dump()
        pet_dict.update(symptoms)
        
        # 3. Predecir con el modelo ML
        logger.info("Realizando predicci√≥n con modelo ML")
        ml_result = await disease_service.predict_disease(pet_dict)
        
        if not ml_result["success"]:
            raise HTTPException(
                status_code=500,
                detail=ml_result.get("message", "Error en predicci√≥n ML")
            )
        
        # 4. Formatear respuesta con Ollama
        logger.info("Formateando respuesta con Ollama")
        conversational_text = ollama_service.format_prediction_response(
            predictions=ml_result["predictions"],
            pet_name=request.pet_name,
            symptoms_mentioned=symptoms_list
        )
        
        return PredictWithOllamaResponse(
            success=True,
            predictions=ml_result["predictions"],
            conversational_response=conversational_text,
            symptoms_detected=symptoms,
            model_info=ml_result["model_info"]
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error en predicci√≥n con Ollama: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Error en predicci√≥n: {str(e)}"
        )

